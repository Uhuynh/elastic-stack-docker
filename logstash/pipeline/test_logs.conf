input {
    file {
        type => "test_log"
        path => ["/usr/share/logstash/data/test.log"]
        start_position => "beginning"
        stat_interval => "1 minute"
#         sincedb_path => "/usr/share/logstash/data/test_log_sincedb.txt"
        sincedb_path => "/dev/null"
        sincedb_write_interval => "1 minute"
        codec => multiline {
          pattern => "^%{TIMESTAMP_ISO8601} "
          negate => true
          what => "previous"
        }
    }
}

filter {
    grok {
        break_on_match => true
        match => {
          "message" => [
            # log pattern to parse
            "%{DATA:create_datetime} %{WORD:level} %{GREEDYDATA:logger_name} > %{GREEDYDATA:msg}"
            ]
        }
    }
    date {
        match => ["timestamp", "YYYY-MM-dd HH:mm:ss"]
        timezone => "UTC"
        # remove redundant field as we overwrite the default @timestamp field
        remove_field => "timestamp"
    }
}

output {
#     stdout { codec => rubydebug }
#     if [level] == "ERROR" {
#         http {
#             id => "dagster_services_id"
#             url => 'http://host.docker.internal:34000/api/notifier/v1/columbo'
#             http_method => post
#             # retry failed events
#             # retry_non_idempotent => true
#             format => json
#             headers => {
#                 "Authorization" => "Token 68f17d096a070d8f4b5a7b8ae52ccf4cacd23a21"
#                 }
#             # set to true to ensure the data is compressed
#             # http_compression => true
#             user => "logstash_test"
#             password => "j5D*95zzTuTo"
# #             cacert => "/etc/ssl/certs/ca-certificates.crt"
#         }
#     }
# }
    elasticsearch {
            hosts => "${ELASTICSEARCH_HOST}"
            cacert => "/usr/share/logstash/config/certs/ca/ca.crt"
            user => "logstash_internal"
            password => "${LOGSTASH_INTERNAL_PASSWORD}"
            index => "logstash-%{type}-index"
        }
}
